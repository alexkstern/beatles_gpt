{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJFaNgQ6Xwt"
      },
      "source": [
        "# Exploring a forward pass through a Transformer (with the Beatles)\n",
        "\n",
        "Number of lines of code: 189\n",
        "\n",
        "Number of words: 2498"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVHuoiQWrFOj"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "This tutorial explores the \"Attention is All You Need\" paper by Vaswani et al., available [here](https://arxiv.org/abs/1706.03762), which introduced the transformer architecture to natural language processing (Vasqani et al.,2017). We will focus on the decoder component of the transformer, guiding you through what goes on under-the-hood. This exploration is based on Andrej Karpathy's practical implementation, detailed in [this Colab Notebook](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing).\n",
        "\n",
        "At the time of this paper's release, Recurrent Neural Networks and Convolutional Neural Networks were the primary frameworks for tasks like language modeling and machine translation. These models, however, faced significant limitations due to their sequential processing nature, which restricted parallelization during training—particularly with longer sequences.\n",
        "\n",
        "In response, the paper introduced the Transformer model which dispenses of recurrence or convolution, and uses attention. This approach was inspired by the success of early encoder-decoder architectures that employed attention mechanisms, such as the work by [(Luong et al., 2015)](https://arxiv.org/abs/1508.04025). Attention allows the Transformer to process different parts of a sequence independently of their positional distances, greatly enhancing parallel processing capabilities. This shift in architecture not only increased computational efficiency but also established new performance benchmarks in machine translation, the field for which the Transformer was initially showcased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz5kmsXsETV9"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"Transformer Model Diagram\" title=\"Transformer Model Overview\" width=\"30%\" height=\"auto\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGxvKSx4Eb2V"
      },
      "source": [
        "The Transformer model features an encoder and decoder, both employing stacked self-attention and fully connected layers. The encoder includes multi-head self-attention and a feed-forward layer, enhanced with residual connections and layer normalization. The decoder mirrors this setup but incorporates a masked multi-head attention layer, ensuring each position only processes preceding information. For machine translation tasks, the encoder-decoder framework is essential as the encoder represents the source text, which the decoder then uses, as well as its own generated tokens, to autoregressively generate subsequent tokens in the target language.\n",
        "\n",
        "In contrast, tasks that involve generating text without translation, such as language modeling, can effectively utilize just the decoder component. This approach is exemplified by early state-of-the-art models like [GPT-2](https://huggingface.co/learn/nlp-course/chapter1/6). Therefore, this tutorial concentrates on the decoder aspect of the architecture, with a specific focus on implementing attention and feed-forward modules.\n",
        "\n",
        "We use a dataset of [Beatles lyrics](https://www.kaggle.com/datasets/jenlooper/beatles-lyrics), which if given the time to train, would build a model that is able to autoregressively generate text in the style of The Beatles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lox_g0pB92GY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z0Bk0wcqEPqJ"
      },
      "outputs": [],
      "source": [
        "device='cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzWr3htonTkm"
      },
      "source": [
        "# Section 1: Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArjTAtm6nY-2"
      },
      "source": [
        "The initial step in building a transformer involves tokenizing our text, which means converting characters into numerical values that the model can process. While tokenization often uses sub-words, this tutorial employs character-level tokenization, where each character in our Beatles lyrics string is mapped to a unique number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n3uXNyJbj8SC"
      },
      "outputs": [],
      "source": [
        "beatles_txt_pth='./beatles.txt'\n",
        "beatles_txt_ob = open(beatles_txt_pth, \"r\")\n",
        "beatles_lyrics= beatles_txt_ob.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIQLQnHHRcXU",
        "outputId": "a658ba5f-2ef0-4cea-b163-abd7c46c8255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Characters: \n",
            " !&'(),-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWY[]abcdefghijklmnopqrstuvwxyzöü‘’“”\n",
            "Number of Unique Characters:85\n",
            "For example Token 50 represents 'Y'\n"
          ]
        }
      ],
      "source": [
        "# Extract unique characters and define vocabulary size\n",
        "chars = sorted(set(beatles_lyrics))\n",
        "vocab_size=len(chars)\n",
        "print(f\"Unique Characters: {''.join(chars)}\\nNumber of Unique Characters:{len(chars)}\")\n",
        "# Create bi-directional mappings between characters and integers\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "print(f\"For example Token 50 represents '{int_to_char[50]}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vx_4HhWU_se"
      },
      "source": [
        "Next, we must encode our entire beatles_lyrics string and turn it into a tensor to then create our training and validation tensors. These tensors will be used to create our train and vali loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3LA4mE6Igaja"
      },
      "outputs": [],
      "source": [
        "# Define encoding and decoding functions\n",
        "def encode(string_text):\n",
        "    return [char_to_int[ch] for ch in string_text]\n",
        "def decode(encoded_text):\n",
        "    return ''.join(int_to_char[i] for i in encoded_text)\n",
        "# Encode the entire Beatles lyrics dataset into a torch.Tensor\n",
        "data = torch.tensor(encode(beatles_lyrics), dtype=torch.long)\n",
        "split_point = int(0.9 * len(data)) # Split data into 90% training and 10% validation\n",
        "train_data = data[:split_point]  # Training data slice\n",
        "val_data = data[split_point:]    # Validation data slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCHAzkTJXMVW"
      },
      "source": [
        "The `CharacterDataset` class processes a dataset using a specified `block_size`, which defines the sequence length for training samples. The `__len__` method ensures all sequences are complete by adjusting the dataset size, while the `__getitem__` method fetches sequences, each comprising an input sequence (`x`) and a target sequence (`y`)—the latter being the input shifted by one token for next token prediction.\n",
        "\n",
        "We set `block_size` to 8, representing the maximum context length for generating sequences autoregressively. DataLoaders (`train_loader` and `val_loader`) are configured with a `batch_size` of 4 to batch and shuffle training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UZdx7uPYrH-",
        "outputId": "2c297e55-f8ec-49cb-a522-3f61cbeae57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 0 element in train data\n",
            "x: tensor([27,  1, 56, 53, 77,  1, 61, 66]) represents \"A day in\"\n",
            "y: tensor([ 1, 56, 53, 77,  1, 61, 66,  1]) represents \" day in \"\n",
            "----\n",
            "Index 1 element in train data\n",
            "x: tensor([ 1, 56, 53, 77,  1, 61, 66,  1]) represents \" day in \"\n",
            "y: tensor([56, 53, 77,  1, 61, 66,  1, 72]) represents \"day in t\"\n",
            "----\n",
            "Index 2 element in train data\n",
            "x: tensor([56, 53, 77,  1, 61, 66,  1, 72]) represents \"day in t\"\n",
            "y: tensor([53, 77,  1, 61, 66,  1, 72, 60]) represents \"ay in th\"\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size # Subtract block_size to avoid overflow, ensure last sequence has full length\n",
        "    def __getitem__(self, idx): # idx is the start index of the sequence and target sequence is shifted by one character\n",
        "        return (self.data[idx:idx+self.block_size], self.data[idx+1:idx+self.block_size+1])\n",
        "block_size = 8 #Define Block Size, meaning sequence lenght size is 8\n",
        "batch_size = 4\n",
        "# Instantiate the Dataset and define DataLoader for training and validation datasets\n",
        "train_dataset = CharacterDataset(train_data, block_size)\n",
        "val_dataset = CharacterDataset(val_data, block_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "# Print the first 3 elements in Train Dataset.\n",
        "for ith in range(3):\n",
        "    x, y = train_dataset[ith]\n",
        "    x_decoded = decode([i.item() for i in x])\n",
        "    y_decoded = decode([i.item() for i in y])\n",
        "    print(f\"Index {ith} element in train data\\nx: {x} represents \\\"{x_decoded}\\\"\\ny: {y} represents \\\"{y_decoded}\\\"\\n----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSWgRYHycVp_"
      },
      "source": [
        "Hence, each batch contains four input and target tensors. This means the shape of our initial input tensor's is `(4, 8)`. With our loaders now prepared, we're ready to conduct a forward pass. The specific batch we'll use is defined below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJct_r3fcAs-",
        "outputId": "c83f7b81-47a6-4468-a330-3259ca20ec5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tutorial x input tensor with batch_size=4 and block_size=8:\n",
            "tensor([[57,  1, 72, 67,  1, 63, 66, 67],\n",
            "        [60, 53, 72,  1, 75, 57,  1, 55],\n",
            "        [54, 70, 67, 75, 66,  7,  1, 77],\n",
            "        [ 1, 64, 61, 72, 72, 64, 57,  1]])\n",
            "Size: torch.Size([4, 8])\n",
            "Corresponding target tensor:\n",
            "tensor([[ 1, 72, 67,  1, 63, 66, 67, 75],\n",
            "        [53, 72,  1, 75, 57,  1, 55, 53],\n",
            "        [70, 67, 75, 66,  7,  1, 77, 57],\n",
            "        [64, 61, 72, 72, 64, 57,  1, 59]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)  # Ensure reproducibility\n",
        "x_example, y_example = next(iter(train_loader))\n",
        "print(f\"Tutorial x input tensor with batch_size=4 and block_size=8:\\n{x_example}\\nSize: {x_example.shape}\")\n",
        "print(f\"Corresponding target tensor:\\n{y_example}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjSjC2tZnp8t"
      },
      "source": [
        "# Section 2: Token and Position embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWSek9Z5KENt"
      },
      "source": [
        "In the next step, we extract embeddings for each token from an embedding table.\n",
        "\n",
        "1. In this token embedding table, each row corresponds to a different token, and each column within a row represents a feature of that token's embedding. Thus, the entire row forms the token's embedding vector with a dimensionality equal to n_embd which we decide is 32. Hence the token embedding table is 32 rows by 85 columns. Pytorch automatically randomly initializes the token embedding table which mimics conditions at the start of training.\n",
        "2. For each token in our input data, the corresponding embedding is plucked from the table and appended to our tensor in its third dimension. Hence the shape of our input tensor is now `(4, 8, 32)`. This means that each individual sequence within each batch has shape `(8, 32)`. This can be seen below for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E07LRf4zJAyu",
        "outputId": "f3b0778e-59d1-4119-dcf8-d9cc1a0a07e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of token_embedding is torch.Size([4, 8, 32])\n",
            "The first sequence of 8 tokens looks like the following:\n",
            "tensor([[ 3.5992e-02, -8.7966e-01, -9.8009e-01,  1.6861e+00,  2.3678e-01,\n",
            "          1.5649e+00, -2.2334e-01,  1.7531e-01,  8.5940e-02,  3.8752e-01,\n",
            "         -1.1794e+00,  1.5783e+00, -2.0817e-01, -4.8517e-01, -4.3715e-02,\n",
            "         -1.1596e-01,  7.9778e-01, -2.4252e-01, -4.7606e-01, -3.7957e-01,\n",
            "         -1.1423e-02, -4.5123e-01,  1.0632e+00,  2.4969e-01, -5.4549e-02,\n",
            "          8.1292e-01, -9.5756e-01,  1.2139e+00, -5.7249e-01,  7.9329e-02,\n",
            "         -1.1229e+00, -1.4157e+00],\n",
            "        [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
            "          1.1241e-01,  6.4079e-01,  4.4116e-01, -1.0231e-01,  7.9244e-01,\n",
            "         -2.8967e-01,  5.2507e-02,  5.2286e-01,  2.3022e+00, -1.4689e+00,\n",
            "         -1.5867e+00, -6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01,\n",
            "         -2.3034e-01, -3.9175e-01,  5.4329e-01, -3.9516e-01, -4.4622e-01,\n",
            "          7.4402e-01,  1.5210e+00,  3.4105e+00, -1.5312e+00, -1.2341e+00,\n",
            "          1.8197e+00, -5.5153e-01],\n",
            "        [ 2.2449e-01,  2.1786e-01, -9.2574e-01,  3.4475e-01, -1.4457e+00,\n",
            "         -9.3554e-01, -4.2728e-01, -2.0108e+00,  5.2931e-02, -2.4355e-01,\n",
            "         -1.5032e+00,  5.8249e-01, -1.3862e+00,  1.1215e+00,  1.3348e+00,\n",
            "          7.1185e-01, -7.7944e-01, -1.4360e+00, -1.5518e+00,  2.2865e-01,\n",
            "          1.2596e-01,  1.0637e+00, -1.2337e+00, -1.0669e+00, -7.9054e-02,\n",
            "         -1.8639e+00, -1.7090e+00,  1.0636e+00,  3.1731e-01,  3.6880e-01,\n",
            "          3.0442e-02, -1.5671e+00],\n",
            "        [ 1.4152e-03, -1.0187e+00, -9.4848e-01,  7.0208e-01, -6.7636e-02,\n",
            "          1.0805e+00, -5.5534e-01, -1.4103e+00, -7.6077e-01, -4.9979e-01,\n",
            "         -9.3943e-01,  6.1541e-01, -2.1970e-01,  1.6174e-02,  1.9803e+00,\n",
            "          1.5285e+00, -1.9009e+00,  3.9448e-01,  5.8052e-01,  2.0588e-02,\n",
            "          4.1966e-01, -7.7630e-01, -1.2459e+00,  6.3344e-02,  6.5097e-01,\n",
            "         -1.0831e+00,  5.4642e-01,  1.3304e+00, -4.2370e-02, -5.6026e-01,\n",
            "          6.9471e-01, -1.1198e+00],\n",
            "        [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
            "          1.1241e-01,  6.4079e-01,  4.4116e-01, -1.0231e-01,  7.9244e-01,\n",
            "         -2.8967e-01,  5.2507e-02,  5.2286e-01,  2.3022e+00, -1.4689e+00,\n",
            "         -1.5867e+00, -6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01,\n",
            "         -2.3034e-01, -3.9175e-01,  5.4329e-01, -3.9516e-01, -4.4622e-01,\n",
            "          7.4402e-01,  1.5210e+00,  3.4105e+00, -1.5312e+00, -1.2341e+00,\n",
            "          1.8197e+00, -5.5153e-01],\n",
            "        [-8.3626e-01, -1.8354e+00,  4.7650e-01, -1.2339e-01, -2.5574e-01,\n",
            "          3.1729e-01, -8.4056e-01,  7.9187e-01, -7.0423e-01, -1.9531e-01,\n",
            "         -2.7726e-01,  5.8296e-01,  2.9672e-01, -5.7120e-01,  8.5577e-01,\n",
            "         -1.0663e-01, -5.9347e-01, -1.1658e+00,  5.6607e-01,  1.6228e+00,\n",
            "         -6.4596e-02, -6.2219e-01,  8.6774e-01, -2.8576e-01,  6.0430e-01,\n",
            "          7.3976e-02, -8.0037e-01, -9.7283e-02, -1.5325e+00, -3.8123e-01,\n",
            "         -1.6687e+00,  1.0869e+00],\n",
            "        [-1.1870e+00, -8.2207e-01,  6.0512e-01, -9.9046e-01, -2.5339e-01,\n",
            "          5.2966e-02,  1.6567e+00,  4.4885e-01,  2.5201e-01, -2.2691e+00,\n",
            "          1.1179e+00,  2.1341e-01,  3.0877e-01, -7.2476e-01, -1.0970e+00,\n",
            "         -1.0562e+00,  1.6772e+00,  2.9700e-01,  7.7051e-01, -1.3406e-01,\n",
            "          1.9057e-01,  1.4081e+00,  5.6129e-01,  8.1946e-01, -1.1448e+00,\n",
            "          1.4498e-01, -4.3010e-01, -1.2729e+00,  3.4685e-01, -1.9059e-01,\n",
            "          2.5113e-01,  1.3542e+00],\n",
            "        [ 1.4152e-03, -1.0187e+00, -9.4848e-01,  7.0208e-01, -6.7636e-02,\n",
            "          1.0805e+00, -5.5534e-01, -1.4103e+00, -7.6077e-01, -4.9979e-01,\n",
            "         -9.3943e-01,  6.1541e-01, -2.1970e-01,  1.6174e-02,  1.9803e+00,\n",
            "          1.5285e+00, -1.9009e+00,  3.9448e-01,  5.8052e-01,  2.0588e-02,\n",
            "          4.1966e-01, -7.7630e-01, -1.2459e+00,  6.3344e-02,  6.5097e-01,\n",
            "         -1.0831e+00,  5.4642e-01,  1.3304e+00, -4.2370e-02, -5.6026e-01,\n",
            "          6.9471e-01, -1.1198e+00]], grad_fn=<SelectBackward0>)\n",
            "Size of the tensor above is torch.Size([8, 32]).\n"
          ]
        }
      ],
      "source": [
        "n_embd = 32 # Set embedding dimension\n",
        "torch.manual_seed(0)\n",
        "token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd) # Initialize the embedding layer with random weights\n",
        "token_embedding = token_embedding_table(x_example) # Apply the embedding to the input tensor\n",
        "print(f\"Size of token_embedding is {token_embedding.shape}\")\n",
        "print(f\"The first sequence of 8 tokens looks like the following:\\n{token_embedding[0]}\\nSize of the tensor above is {token_embedding[0].shape}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LrdWJRSQBf3"
      },
      "source": [
        "The next step involves incorporating positional embeddings. Similar to the token embeddings, we utilize a `position_embedding_table`. Using `torch.arange` to generate indices from 0 up to `sequence_length-1`, we expand these indices across the batch size to ensure each input in the batch receives a corresponding position embedding. The resulting shapes of the positional embeddings match those of the token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oega83M9RRej"
      },
      "outputs": [],
      "source": [
        "batch_size, sequence_length = x_example.shape # x_example shape is [batch_size, sequence_length]\n",
        "position_embedding_table = nn.Embedding(num_embeddings=sequence_length, embedding_dim=n_embd) # Create a position embedding table\n",
        "positions = torch.arange(sequence_length, device=device).expand(batch_size, -1)\n",
        "pos_emb = position_embedding_table(positions) # Retrieve position embeddings using the indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UgS6Qi2O4_B"
      },
      "source": [
        "Before feeding the input batch into the Transformer, we combine the token and position embeddings through element-wise addition, since both have dimensions of `(4, 8, 32)`. After this pooling, we normalize along the embedding dimension to consolidate the information. This process merges the semantic and positional data of the tokens, producing an integrated input tensor `x` of size `(4, 8, 32)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VavdOKa4UUDE",
        "outputId": "b4df065f-e085-499a-e114-8c340cdf0ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example token embedding in x before layer norm:\n",
            "tensor([ 0.6451,  0.6672, -1.5585,  3.8984,  1.3934,  0.4970,  0.2501,  0.6093,\n",
            "         0.5185, -1.8517, -0.3389,  1.7525, -0.5139, -0.5475,  0.7616,  0.2294,\n",
            "         0.6061, -0.4820, -2.4072, -1.4543, -0.1333, -0.4006,  0.3217,  0.0206,\n",
            "        -0.4155,  1.0770,  0.5796,  2.8171, -1.8143, -0.3091, -1.7327, -1.3206],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Example token embedding in x  after layer norm:\n",
            "tensor([ 0.4568,  0.4735, -1.2140,  2.9235,  1.0241,  0.3445,  0.1573,  0.4297,\n",
            "         0.3608, -1.4363, -0.2893,  1.2965, -0.4220, -0.4474,  0.5452,  0.1416,\n",
            "         0.4272, -0.3978, -1.8575, -1.1350, -0.1334, -0.3360,  0.2116, -0.0167,\n",
            "        -0.3473,  0.7843,  0.4071,  2.1037, -1.4080, -0.2667, -1.3461, -1.0336],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_pre_norm = token_embedding + pos_emb\n",
        "layer_norm = nn.LayerNorm(n_embd) # Applying layer normalization pytorch module to the combined embeddings\n",
        "x = layer_norm(x_pre_norm)\n",
        "print(f\"Example token embedding in x before layer norm:\\n{x_pre_norm[0][0]}\\n\\nExample token embedding in x  after layer norm:\\n{x[0][0]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jMwkjz2p04r"
      },
      "source": [
        "# Section 3: Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqgcKDO5EHnv"
      },
      "source": [
        "##Section 3a: One head of Self Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG83ntiWp34B"
      },
      "source": [
        "Next, we explore a single head of self-attention. Self-attention allows the model to weigh the importance of different parts of the input data relative to each other. Here, we'll explore how this process is implemented in our `Head` class and how it processes input to produce an output.\n",
        "\n",
        "### Attention\n",
        "\n",
        "Attention is described as:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "**Query (Q)**:\n",
        "- The query vector  represents a specific item or part of the input data which is actively seeking information. It can be thought of as a question posed by a particular part of the input data. In the context of self-attention, each token in the sequence generates a query vector that seeks to find out how much it should pay attention to other parts of the input data.\n",
        "\n",
        "**Key (K)**:\n",
        "- The Key vector represents aspects of the input data to be queried against. Each part of the input data generates a key that corresponds to it. The compatibility of a query with different keys determines the attention or focus level on various parts of the input. This relationship is determined through the scaled dot product function between the the two vectors.\n",
        "\n",
        "**Value (V)**:\n",
        "- The value vector actually contains the data from the input tokens that will be aggregated into the output. The amount of attention a query pays to a particular key determines the weighting of the corresponding value in the output.\n",
        "\n",
        "**Explaining the equation**:\n",
        "1. **Dot Product of Queries and Keys**: The attention scores are calculated by taking the dot product of the query with all keys, quantifying the similarity between tokens. The 'similarity' practically means how much each token of the input sequence influences others, guiding how much attention is allocated to each token.\n",
        "2. **Scaling**: The dot products are scaled down by the square root of the dimension of the keys $\\sqrt{d_k}$ to normalize the values.\n",
        "3. **Softmax Application**: The softmax function is applied to the scaled dot product scores to convert them into values between 0 and 1, effectively representing probabilities. This  ensures that the more similar the query is to a key, the higher the attention score.\n",
        "4. **Multiplication by Values**: Finally, the weighted softmax scores are applied to the value vectors. Each value vector represents the embedding of a token, encapsulating its contextual information. Multiplying these vectors by the softmax scores effectively weights the importance of each token's contribution. This process highlights which aspects of the embeddings are most relevant, ensuring that the model focuses on the most informative parts of the input data during further processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dDt1bq9UoxGk"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):#One Head of Self Attention\n",
        "    def __init__(self,head_size,dropout):\n",
        "        super().__init__()\n",
        "        self.key=nn.Linear(n_embd,head_size,bias=False) #takes a tensor of length n_embd and projects it to head_size\n",
        "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) #Implements masking\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        #Initialize values for Tutorial demonstration purposes\n",
        "        self.key_vector = None\n",
        "        self.query_vector = None\n",
        "        self.unmasked_weights = None\n",
        "        self.masked_weights = None\n",
        "        self.softmax_weights = None\n",
        "        self.dropout_weights = None\n",
        "        self.aggregated_values = None\n",
        "    def forward(self,x):\n",
        "        B,T,C=x.shape #B=batch_size, T=block_size, C=n_embd\n",
        "        k=self.key(x)\n",
        "        self.key_vector = k.clone()\n",
        "        q=self.query(x)\n",
        "        self.query_vector = q.clone()\n",
        "        transpose_k=torch.transpose(k,1,2)\n",
        "        wei= q@transpose_k * C**-0.5\n",
        "        self.unmasked_weights = wei.clone()\n",
        "        wei=wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        self.masked_weights = wei.clone()\n",
        "        wei=F.softmax(wei,dim=-1)\n",
        "        self.softmax_weights = wei.clone()\n",
        "        wei=self.dropout(wei)\n",
        "        self.dropout_weights = wei.clone()\n",
        "        v=self.value(x)\n",
        "        self.aggregated_values = v.clone()\n",
        "        return wei@v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsommu7Vi7Kd"
      },
      "source": [
        "We will provide step-by-step outputs of the computations in the attention-head. However, for clarity, we will display the martices of the first batch sequence. This matrix will always contain 8 embeddings, with tensor shapes that vary throughout the forward pass.\n",
        "\n",
        "The input to our `Head` class is the tensor `x` defined before, shape `(4, 8, 32)`. Since we are only looking at the first batch sequence, this tensor is `(8, 32)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhJvJWOVrb2t",
        "outputId": "f716e297-a660-4489-ec20-ddf073f1d6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the input tensor x: torch.Size([4, 8, 32])\n",
            "Tensor of the first batch of the input tensor x:\n",
            "tensor([[ 0.4568,  0.4735, -1.2140,  2.9235,  1.0241,  0.3445,  0.1573,  0.4297,\n",
            "          0.3608, -1.4363, -0.2893,  1.2965, -0.4220, -0.4474,  0.5452,  0.1416,\n",
            "          0.4272, -0.3978, -1.8575, -1.1350, -0.1334, -0.3360,  0.2116, -0.0167,\n",
            "         -0.3473,  0.7843,  0.4071,  2.1037, -1.4080, -0.2667, -1.3461, -1.0336],\n",
            "        [-0.8623,  0.7000, -0.4710,  1.5819, -0.2941, -0.9924,  0.4580,  1.0163,\n",
            "         -0.8155,  0.6536, -0.6400,  1.3092,  0.0311,  2.1863, -1.5589, -1.2425,\n",
            "          0.1511,  0.7807,  0.1113, -0.0855,  0.2623,  0.5294, -0.0995, -0.8968,\n",
            "         -1.0212, -0.3187,  0.9326,  1.2058, -0.9879, -1.5286,  1.5585, -1.6531],\n",
            "        [ 1.0980,  1.0847, -0.4264,  1.8365, -0.4578, -0.2888,  0.7239, -1.4357,\n",
            "          0.6711,  0.1065, -0.2511,  0.3691, -0.3241,  0.3483,  0.2968,  1.6777,\n",
            "         -0.8705, -1.0669, -1.8113, -0.1857,  0.1032,  2.0222, -1.7120, -1.1176,\n",
            "          0.0886, -1.3562, -1.0356,  0.5925,  1.3513, -0.0800,  0.6570, -0.6078],\n",
            "        [ 0.1109, -0.8744, -0.7960, -0.1980, -0.6663,  0.6807, -1.0689, -2.1778,\n",
            "         -0.7508, -0.0661,  0.0947,  0.7901, -0.1097,  1.1313,  1.8664,  1.0986,\n",
            "         -2.0000,  1.7200,  1.6136, -0.9897, -0.2613, -0.6684, -0.1878,  0.3401,\n",
            "         -0.6433,  0.1445,  0.9532,  0.8476,  0.5317,  0.5119,  0.5798, -1.5567],\n",
            "        [-0.8492,  0.3010, -0.5866,  1.0663, -0.0926,  0.1141,  0.1631, -0.6694,\n",
            "         -1.0270,  1.2132, -0.9608, -0.8083,  0.3402,  0.9013, -0.3429, -2.1129,\n",
            "          0.6230,  0.1520, -0.3750,  0.3153, -0.9493, -0.8504,  0.2947,  0.0890,\n",
            "         -0.1725,  0.2752,  2.1912,  2.9966, -1.3207, -0.7779,  0.9874, -0.1282],\n",
            "        [ 0.3158, -0.0683, -0.1358,  0.3913, -0.6897,  0.9549, -0.8317,  1.7468,\n",
            "         -0.4220,  0.4917, -0.5943, -0.1130,  0.9382, -2.7657,  0.8037, -0.6266,\n",
            "         -0.7664, -0.7724, -0.4033,  1.9233, -1.2296, -0.5958, -0.2244,  1.0692,\n",
            "          0.3037,  0.6282, -1.0415,  1.0137, -0.8656,  0.8315, -1.0157,  1.7498],\n",
            "        [-0.8637, -0.7862,  1.3994, -1.0434, -0.4625,  0.7589,  1.0445,  0.9464,\n",
            "         -1.1989, -1.5396,  0.3256,  1.3749,  0.2122,  0.9439, -0.5027, -0.8989,\n",
            "          2.3345,  0.0474,  0.7366, -0.4555,  0.8191,  0.7000, -0.2454,  0.3276,\n",
            "         -1.6781, -0.8694, -1.2414, -1.0887,  1.3893, -0.9469, -0.1231,  0.5841],\n",
            "        [ 0.1941, -0.2422,  0.9247,  1.1657, -0.6326,  0.6037, -0.9708, -0.6058,\n",
            "         -1.2425,  0.0299, -1.1455, -0.4416, -0.1142,  0.9764,  1.1013,  1.9405,\n",
            "         -0.9885,  0.0865,  1.0127,  0.0887,  1.5174, -1.8996, -1.6285,  0.0350,\n",
            "         -0.8091, -1.2524, -0.4080,  0.9657,  0.4967, -0.8398,  1.9341,  0.1481]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Shape of first batch of the input tensor x: torch.Size([8, 32])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of the input tensor x: {x.shape}\\nTensor of the first batch of the input tensor x:\\n{x[0]}\\nShape of first batch of the input tensor x: {x[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hqsLTkXoqVD2"
      },
      "outputs": [],
      "source": [
        "num_heads = 4\n",
        "head_size = n_embd // num_heads\n",
        "dropout = 0.2\n",
        "head = Head(head_size,dropout) # Create an instance of the Head\n",
        "output = head(x) # Forward pass of x through the head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJBA13IblEk9"
      },
      "source": [
        "**Linear Projections**:\n",
        "   - **Key and Query Transformations**: The input tensor `x` first undergoes two separate linear transformations to produce keys (`k`), queries (`q`). These transformations project the input tensor from an embedding dimension of `n_embd` to a smaller dimension called `head_size`. `head_size= n_embd/num_heads` where `num_heads` is another hyperparameter. `num_heads` refers to the number of attention heads we want to define for our multihead attention class. We will go into details of this later. For now, let `num_heads = 4` which implies `head_size= 32/4 =8`.\n",
        "    - **Example**: Our embedding dimension (`n_embd`) is 32 and our `head_size` is 8. If the input tensor `x` has a shape of `(4, 8, 32)`, the linear projection transforms these embeddings by applying a weight matrix of shape `(32, 8)` to each batch. Consequently, each sequence in the batch is projected to a new shape of `(8, 8)`. This results in the transformed tensors for keys (`k`), and queries (`q`), each having the shape `(4, 8, 8)`.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1wAuQQarNKZ",
        "outputId": "b5c3d7a6-7606-4fde-bfbb-6df0e246bb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Key vector:  torch.Size([4, 8, 8]) \n",
            "Key vector of first batch sequence: tensor([[ 2.2918e-01,  3.3472e-01, -2.7254e-01,  4.7843e-01,  6.8547e-01,\n",
            "          9.1177e-01,  2.4008e-01, -1.1264e+00],\n",
            "        [ 1.9337e-01,  3.3117e-01,  1.4469e-01,  8.1046e-01, -8.5312e-01,\n",
            "         -8.7008e-02,  7.1845e-01, -7.1209e-02],\n",
            "        [ 6.8281e-01, -4.2687e-01, -4.9447e-01,  5.3350e-01,  1.8807e-01,\n",
            "          1.3283e+00, -3.9638e-04,  6.1308e-02],\n",
            "        [ 6.6354e-01,  1.1197e-01,  3.9720e-01,  5.7539e-01, -3.3714e-01,\n",
            "          4.7082e-03,  3.1838e-01,  8.2698e-01],\n",
            "        [ 6.1830e-01, -3.4168e-01, -2.0562e-01,  4.6383e-01,  2.0837e-01,\n",
            "         -8.4284e-01,  1.1451e+00, -1.9244e-01],\n",
            "        [-7.0144e-01,  5.8017e-02, -6.6834e-01, -3.2742e-02,  2.5882e-01,\n",
            "          4.3821e-01, -9.6909e-01, -5.1473e-01],\n",
            "        [ 3.3577e-01,  4.5456e-01,  9.7191e-01, -8.0475e-01, -1.4307e-01,\n",
            "         -2.5993e-02, -1.3257e+00,  5.5653e-01],\n",
            "        [ 1.6220e+00,  3.0541e-01,  4.6095e-01,  2.6286e-01,  5.5144e-01,\n",
            "          5.5753e-01, -6.5516e-01, -4.4850e-01]], grad_fn=<SelectBackward0>) \n",
            "Shape of Key vector of first batch sequence: torch.Size([8, 8])\n",
            "Shape of Query vector:  torch.Size([4, 8, 8]) \n",
            "Query vector of first batch sequence: tensor([[-0.1717, -0.2242,  0.3787,  0.1828, -0.2792, -0.5374,  0.3613, -0.5644],\n",
            "        [-0.8020,  0.1893,  1.4122,  0.0337, -0.8932,  0.2804,  0.5624, -0.3740],\n",
            "        [-0.3718, -0.1137,  0.8543, -0.6273, -0.4741,  0.3008, -0.5536, -0.2802],\n",
            "        [ 0.5760,  0.3553,  1.1753,  0.1642,  0.7993, -0.1789, -1.0658,  0.4980],\n",
            "        [-1.1075,  1.2397,  1.2823,  0.7764, -0.2806, -0.4919, -0.1482, -0.1392],\n",
            "        [ 0.0739,  0.4530, -1.2278,  0.9452, -0.2687, -0.1484,  0.7201, -0.8366],\n",
            "        [ 0.7550, -1.0155, -0.8379, -0.7294, -0.2090,  0.5132,  0.4406,  0.3339],\n",
            "        [ 0.1279,  0.5496,  0.7074, -0.3484, -1.0171,  0.8609, -0.9792,  0.1204]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "Shape of Query vector of first batch sequence: torch.Size([8, 8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of Key vector: \", head.key_vector.shape,\"\\nKey vector of first batch sequence:\", head.key_vector[0],\"\\nShape of Key vector of first batch sequence:\", head.key_vector[0].shape)\n",
        "print(\"Shape of Query vector: \", head.query_vector.shape,\"\\nQuery vector of first batch sequence:\", head.query_vector[0],\"\\nShape of Query vector of first batch sequence:\", head.query_vector[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85dzFE1ZnyOI"
      },
      "source": [
        "**Calculating Attention Scores**:\n",
        "Now we calculate the weight attention matrix, i.e. the weights we want to eventually apply to our Value matrix. This is implemented in the class with this equation: $$\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
        "\n",
        "   - **Transpose**: Before calculating the dot product, the keys tensor `k` is transposed to align the dimensions properly for matrix multiplication with the query tensor `q`.\n",
        "   - **Dot Product and Scaling**:Attention scores are computed by taking the dot product between queries `q` and the transposed keys `k` and then scaling down by dividing by the square root of the embedding dimension (`8`).\n",
        "   - **Shape of Scores**: The resulting tensor, denoted as `wei` in our class, which holds the attention scores, has a shape of `(4, 8, 8)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9cddpjLnxhQ",
        "outputId": "057e46c6-431f-4d89-99ae-bfa1ea9f6663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Unmasked weight vector: torch.Size([4, 8, 8]) \n",
            "Unmasked weight vector of first batch sequence: tensor([[-0.0157,  0.1203, -0.1613, -0.0254,  0.1581, -0.0918, -0.1198, -0.0993],\n",
            "        [-0.0512,  0.2311, -0.1993,  0.0427, -0.0958, -0.1471,  0.0581, -0.1980],\n",
            "        [-0.0927, -0.0873, -0.1183, -0.0933, -0.2811,  0.0696,  0.3176, -0.0025],\n",
            "        [-0.0748, -0.1653, -0.0545,  0.1389, -0.1644, -0.0476,  0.5207,  0.4320],\n",
            "        [-0.0595,  0.2116, -0.3924,  0.0513, -0.1412, -0.0188,  0.1741, -0.1577],\n",
            "        [ 0.3095,  0.2778,  0.1183, -0.0383,  0.2893,  0.0640, -0.5482, -0.0684],\n",
            "        [-0.0412, -0.0842,  0.2894,  0.0219,  0.1082, -0.0765, -0.1445,  0.0122],\n",
            "        [-0.0758,  0.0190,  0.0490,  0.0639, -0.4415,  0.0852,  0.4859,  0.1974]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "Shape of Unmasked weight vector of first batch sequence: torch.Size([8, 8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of Unmasked weight vector:\", head.unmasked_weights.shape, \"\\nUnmasked weight vector of first batch sequence:\", head.unmasked_weights[0], \"\\nShape of Unmasked weight vector of first batch sequence:\", head.unmasked_weights[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHvYpSjrCEG"
      },
      "source": [
        "**Masking, Normalization, and Dropout**:\n",
        "\n",
        "- **Masking**: To prevent the model from accessing future tokens, we apply a masking operation using a lower triangular matrix (`tril`), setting attention scores for future tokens to negative infinity (`-inf`). This ensures that during the softmax operation, as seen below, the influence of future tokens is effectively zeroed out, as $ e^{-\\infty} $ approaches zero.\n",
        "\n",
        "- **Softmax**: Post-masking with (`-inf`), the softmax function normalizes the scores across each row to form a probability distribution by using the formula:\n",
        "  $$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
        "\n",
        "- **Dropout**: This step introduces regularization by randomly setting a percentage of the softmax output elements to zero, reducing overfitting. The remaining elements are scaled up during training to maintain the overall activation level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FiRazFjVKT1",
        "outputId": "112b95f7-d5e7-4100-c0a5-d013cb5e49c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Masked weight vector:\n",
            " torch.Size([4, 8, 8]) \n",
            "Masked weight vector of first batch sequence:\n",
            " tensor([[-0.0157,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0512,  0.2311,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0927, -0.0873, -0.1183,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0748, -0.1653, -0.0545,  0.1389,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0595,  0.2116, -0.3924,  0.0513, -0.1412,    -inf,    -inf,    -inf],\n",
            "        [ 0.3095,  0.2778,  0.1183, -0.0383,  0.2893,  0.0640,    -inf,    -inf],\n",
            "        [-0.0412, -0.0842,  0.2894,  0.0219,  0.1082, -0.0765, -0.1445,    -inf],\n",
            "        [-0.0758,  0.0190,  0.0490,  0.0639, -0.4415,  0.0852,  0.4859,  0.1974]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "Shape of Masked weight vector of first batch sequence: torch.Size([8, 8]) \n",
            "\n",
            "Softmax applied to Masked weight vector of first batch sequence:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4299, 0.5701, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3356, 0.3374, 0.3271, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2397, 0.2189, 0.2446, 0.2968, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1974, 0.2588, 0.1415, 0.2205, 0.1819, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1900, 0.1841, 0.1569, 0.1342, 0.1862, 0.1486, 0.0000, 0.0000],\n",
            "        [0.1344, 0.1287, 0.1870, 0.1431, 0.1560, 0.1297, 0.1212, 0.0000],\n",
            "        [0.1073, 0.1180, 0.1216, 0.1234, 0.0744, 0.1261, 0.1882, 0.1410]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "\n",
            "Applying Dropout to the matrix above: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.7126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4194, 0.0000, 0.4088, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.2737, 0.0000, 0.3710, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2467, 0.3235, 0.1768, 0.2756, 0.2273, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2375, 0.2301, 0.0000, 0.1677, 0.0000, 0.1858, 0.0000, 0.0000],\n",
            "        [0.1679, 0.1609, 0.2337, 0.0000, 0.1950, 0.1621, 0.1515, 0.0000],\n",
            "        [0.1341, 0.1475, 0.1520, 0.1543, 0.0931, 0.1576, 0.2352, 0.1763]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of Masked weight vector:\\n\", head.masked_weights.shape,\"\\nMasked weight vector of first batch sequence:\\n\", head.masked_weights[0],\"\\nShape of Masked weight vector of first batch sequence:\", head.masked_weights[0].shape,\n",
        "      \"\\n\\nSoftmax applied to Masked weight vector of first batch sequence:\\n\", head.softmax_weights[0],\n",
        "      \"\\n\\nApplying Dropout to the matrix above:\", head.dropout_weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45HuaZDuyrbk"
      },
      "source": [
        "**Creating Value Tensor, Applying Attention, and Output**\n",
        "- The input tensor `x` is transformed through a linear module separate to those for keys (`k`) and queries (`q`), producing a value tensor with shape `(4, 8, 8)`.\n",
        "- This is then combined with the weighted attention scores.\n",
        "- The final output from the `Head` class is a matrix that aggregates information across the input sequence based on the attention scores, resulting in an output shape of `(4, 8, 8)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeNw3agmVzC7",
        "outputId": "7966b78b-d9b9-459f-9fe3-2559c4bc08c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Value tensor:  torch.Size([4, 8, 8]) \n",
            "Value tensor of first batch sequence:\n",
            " tensor([[-0.4804,  0.3331,  0.8757, -0.8767,  0.6813, -0.8240,  0.5069, -0.8390],\n",
            "        [ 0.7429,  0.4216,  0.4471, -0.2993,  0.4419, -0.3827, -0.6488, -1.1264],\n",
            "        [ 0.1403,  0.6073,  0.9829, -0.5851,  1.2902,  0.1106,  0.0901, -0.4593],\n",
            "        [ 0.3313,  0.0216,  0.2271, -0.5255,  0.0763,  0.4787, -0.9156,  0.3430],\n",
            "        [ 1.0087,  0.2155,  0.5385, -0.5209,  0.4164, -0.9702,  0.2153, -0.1101],\n",
            "        [ 0.6059,  0.0109, -0.3678, -0.3594, -0.8629, -0.4883,  0.2519,  0.6358],\n",
            "        [-0.4756, -0.2780, -1.0006,  0.5887,  0.3727, -0.4774, -1.0189, -0.8238],\n",
            "        [ 0.8161, -0.5589, -0.8094, -0.7569,  0.8272, -0.3293, -0.1516,  0.3642]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "Shape of Value tensor of first batch sequence:  torch.Size([8, 8]) \n",
            "\n",
            "Now to get final output we simply perform wei@value. \n",
            "Shape of Output tensor:  torch.Size([4, 8, 8]) \n",
            "Output tensor of first batch sequence:\n",
            " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.5294,  0.3005,  0.3186, -0.2133,  0.3149, -0.2727, -0.4624, -0.8027],\n",
            "        [-0.1442,  0.3880,  0.7691, -0.6070,  0.8132, -0.3004,  0.2494, -0.5397],\n",
            "        [ 0.3262,  0.1234,  0.2066, -0.2769,  0.1492,  0.0729, -0.5172, -0.1810],\n",
            "        [ 0.4672,  0.3809,  0.7195, -0.6799,  0.6549, -0.3962, -0.2723, -0.5831],\n",
            "        [ 0.2250,  0.1818,  0.2806, -0.4320,  0.1159, -0.2942, -0.1357, -0.2828],\n",
            "        [ 0.2945,  0.2674,  0.3426, -0.4028,  0.4848, -0.5147, -0.0697, -0.4726],\n",
            "        [ 0.3389,  0.0604, -0.0181, -0.4318,  0.5007, -0.4139, -0.3619, -0.3352]],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "Shape of Output tensor of first batch sequence:  torch.Size([8, 8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of Value tensor: \", head.aggregated_values.shape,\n",
        "      \"\\nValue tensor of first batch sequence:\\n\", head.aggregated_values[0], \"\\nShape of Value tensor of first batch sequence: \", head.aggregated_values[0].shape,\n",
        "      \"\\n\\nNow to get final output we simply perform wei@value.\", \"\\nShape of Output tensor: \", output.shape,\n",
        "      \"\\nOutput tensor of first batch sequence:\\n\", output[0], \"\\nShape of Output tensor of first batch sequence: \", output[0].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zmr8EZZqVg2"
      },
      "source": [
        "## Section 3b: Multi-Head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOSGDUATEhLL"
      },
      "source": [
        "In this section, we apply the Head module num_heads times in order to employ Multi-Head Attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kUNDRrblpKDJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_heads,head_size,dropout):\n",
        "        super().__init__()\n",
        "        self.heads=nn.ModuleList([Head(head_size,dropout) for _ in range(num_heads)])\n",
        "        self.proj=nn.Linear(n_embd,n_embd)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.individual_heads=None #For tutorial purposes\n",
        "    def forward(self,x):\n",
        "        self.individual_heads= [h(x) for h in self.heads]\n",
        "        out=torch.cat(self.individual_heads,dim=-1)\n",
        "        out=self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "njSbNEDNpKP1"
      },
      "outputs": [],
      "source": [
        "# Apply the multi-head attention module\n",
        "multihead=MultiHeadAttention(num_heads,head_size,dropout)\n",
        "output_multihead = multihead(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsgyX0Ej6jas"
      },
      "source": [
        "Here’s how the Multihead Attention module operates on `x`:\n",
        "\n",
        "1. Each of the four heads processes the input tensor `x` independently, outputting a new tensor of shape `(4, 8, 8)` that focuses on different aspects of the input. We confirm these dimensions for each head's output.\n",
        "\n",
        "2. The outputs from all heads are concatenated along the third dimension to form a unified tensor with a shape of `(4, 8, 32)`.\n",
        "\n",
        "3. This tensor undergoes a linear transformation with a `(32, 32)` weight matrix, maintaining the shape `(4, 8, 32)`. A dropout module is then applied to the tensor for regularization before producing the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigUdlAx4HX5",
        "outputId": "ca7c0c0f-65ef-4297-f67b-ff4ae62be02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head 1: torch.Size([4, 8, 8])\n",
            "Head 2: torch.Size([4, 8, 8])\n",
            "Head 3: torch.Size([4, 8, 8])\n",
            "Head 4: torch.Size([4, 8, 8])\n",
            "\n",
            "Concatenate the four heads in the third dimension creating the following output shape from the Multi-Head Attention module: torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "for i, head in enumerate(multihead.individual_heads):\n",
        "    print(f\"Head {i+1}:\", head.shape)\n",
        "print(\"\\nConcatenate the four heads in the third dimension creating the following output shape from the Multi-Head Attention module:\", output_multihead.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK6u-XHmEG-1"
      },
      "source": [
        "**Residual Connection Implementation and Layer Normalization**:\n",
        "\n",
        "Residual connections improve gradient flow during backpropagation, mitigating the vanishing gradient issue in deep networks. This is implemented by adding the output tensor `output_multihead` of size `(4, 8, 32)` to the original tensor `x`. This addition is followed by a layer normalization to consolidate the combined outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Os49XdnyDPHj"
      },
      "outputs": [],
      "source": [
        "x= x+output_multihead\n",
        "norm=nn.LayerNorm(n_embd)\n",
        "x=norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-_0MNJ1qVmr"
      },
      "source": [
        "# Section 4: Feed Forward Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy-wgd14O8S_"
      },
      "source": [
        "The following contains the Feed Forward module which essentially linear layer's followed by a non-linear activation function (multilayer-perceptron)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HG3LikDtOHPN"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):#Basic linear layer followed by non-linearity\n",
        "    def __init__(self, n_embd, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.expand_linear = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.compress_linear = nn.Linear(4 * n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        #Initialize values for Tutorial demonstration purposes\n",
        "        self.x_initial = None\n",
        "        self.x_expanded_dim = None\n",
        "        self.x_post_relu = None\n",
        "        self.x_compressed_dim = None\n",
        "    def forward(self, x):\n",
        "        self.x_initial = x.clone()\n",
        "        x = self.expand_linear(x)\n",
        "        self.x_expanded_dim = x.clone()\n",
        "        x = F.relu(x)\n",
        "        self.x_post_relu = x.clone()\n",
        "        x = self.compress_linear(x)\n",
        "        self.x_compressed_dim = x.clone()\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nY3wyZytPWmu"
      },
      "outputs": [],
      "source": [
        "ffw= FeedForward(n_embd, dropout)\n",
        "ffw_x_output=ffw(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE_YT7KQO6V2"
      },
      "source": [
        "Using the output `x` from the previous layer, we do the following steps which is demonstrated in the code below too:\n",
        "\n",
        "1. **Dimension Expansion**: A linear layer first expands the dimensionality of `x` from `n_embd` to `4 x n_embd`. The paper explains that expanding the dimension exactly by 4 yielded best results.\n",
        "2. **Activation Function**: A ReLU (Rectified Linear Unit) activation is applied, defined  as $ {ReLU}(x) = \\max(0, x) $. This function sets all negative elements to zero and retains non-negative values unchanged, introducing non-linearity into the model. The impact of ReLU is seen in the comparison below of tensors before and after its application.\n",
        "3. **Dimension Compression**: Another linear layer then compresses the dimensionality back from `4 x n_embd` to `n_embd`, aligning the output dimensions with those of the input to the feedforward block.\n",
        "4. A dropout layer follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMFE6yloSH5W",
        "outputId": "580b95f5-82d6-49c6-9d74-2af5afbbeea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tensor shape: torch.Size([8, 32])\n",
            "Linear transformation of input tensor causing tensor of shape: torch.Size([8, 128])\n",
            "First sequence of batch matrix with increased dim:\n",
            "tensor([[-4.7745e-01,  1.2739e+00, -6.0470e-01,  ...,  1.0150e-01,\n",
            "          2.6331e-01,  4.0180e-01],\n",
            "        [ 1.5667e-01,  7.5898e-01, -9.1495e-01,  ...,  9.9886e-01,\n",
            "          9.5105e-02, -5.3086e-02],\n",
            "        [ 4.7425e-01,  4.4758e-01, -9.2756e-01,  ...,  2.8581e-01,\n",
            "          9.6989e-01, -9.7188e-01],\n",
            "        ...,\n",
            "        [-8.6118e-01, -2.8129e-01,  1.1455e+00,  ..., -3.6359e-01,\n",
            "         -1.2507e-01, -2.0360e-01],\n",
            "        [-1.7872e-04, -2.4844e-02,  1.1270e+00,  ..., -2.8387e-01,\n",
            "         -1.2985e-01,  9.3607e-02],\n",
            "        [-3.6850e-01,  2.4188e-02,  2.0080e-01,  ...,  4.5271e-02,\n",
            "          5.1141e-01, -7.5126e-02]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Apply relu to above matrix:\n",
            "tensor([[0.0000, 1.2739, 0.0000,  ..., 0.1015, 0.2633, 0.4018],\n",
            "        [0.1567, 0.7590, 0.0000,  ..., 0.9989, 0.0951, 0.0000],\n",
            "        [0.4743, 0.4476, 0.0000,  ..., 0.2858, 0.9699, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 1.1455,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 1.1270,  ..., 0.0000, 0.0000, 0.0936],\n",
            "        [0.0000, 0.0242, 0.2008,  ..., 0.0453, 0.5114, 0.0000]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Apply another linear transformation reducing dim back to orginal dim: torch.Size([4, 8, 32])\n",
            "Finally we apply a dropout layer to the compressed_dim tensor.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Input tensor shape: {ffw.x_initial[0].shape}\\nLinear transformation of input tensor causing tensor of shape: {ffw.x_expanded_dim[0].shape}\")\n",
        "print(f\"First sequence of batch matrix with increased dim:\\n{ffw.x_expanded_dim[0]}\\n\\nApply relu to above matrix:\\n{ffw.x_post_relu[0]}\")\n",
        "print(f\"\\nApply another linear transformation reducing dim back to orginal dim: {ffw.x_compressed_dim.shape}\\nFinally we apply a dropout layer to the compressed_dim tensor.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA_Ezkm7RDip"
      },
      "source": [
        "This is followed by implementing another residual connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mjtt3WkWGQ1",
        "outputId": "a144b880-006f-4a62-819f-404a0fd91040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape: torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "x= x+ffw_x_output\n",
        "print(f\"x shape: {x.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ_HaEs4qVq7"
      },
      "source": [
        "This marks the completion of one transformer Block. Typically, this Block is repeated several times, determined by `n_layer`, a hyperparameter. The output from the first Block, the tensor `x`, serves as the input for the subsequent Block, and so on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWavT4V5Sz2R"
      },
      "source": [
        "# Section 5: Calculate Logits and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yECyfNAH4yUB"
      },
      "source": [
        "In this tutorial, we let `n_layers = 1`. Therefore, the final representation of our input batch is the tensor `x` above. Before we proceed to calculate the logits and loss, we must apply a final layer norm to `x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "heQDHN2PS15z"
      },
      "outputs": [],
      "source": [
        "final_norm=nn.LayerNorm(n_embd)\n",
        "x=final_norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE4UD9re6jLK"
      },
      "source": [
        "Next, we apply a linear transformation to expand each token's embedding from `n_embd = 32` to `vocab_size = 85`. This transformation maps each token's representation into a space where each dimension represents the likelihood of a vocabulary token being the next in the sequence. Below, we illustrate this transformation: the first sequence in the batch is reshaped to `(4, 8, 85)`, and we examine the first token vector of this sequence, now with 85 dimensions, to highlight the new token representations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3KO7VSJy5Io"
      },
      "source": [
        "Next, we perform a linear transformation to expand each token's embedding from `n_embd = 32` to `vocab_size = 85`. This step maps each token's representation to a dimensionality where each element reflects the likelihood of a corresponding vocabulary token being next in the sequence. We demonstrate this with the first sequence in the batch reshaped to `(4, 8, 85)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l2spb7A6ict",
        "outputId": "87b1b579-04b1-400d-b1f1-c9e535465599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x after transformation torch.Size([4, 8, 85])\n",
            "Shape of the first sequence of the batch torch.Size([8, 85])\n",
            "Shape of the first token in the first sequence of the batch torch.Size([85])\n",
            "The first token in the first sequence of the batch:\n",
            "tensor([ 0.3369,  0.4889,  0.8188, -1.2534, -0.2159,  0.3670,  0.0453, -0.3863,\n",
            "         0.8111,  0.8527, -0.4549,  0.1976,  0.7870, -0.5994,  0.1839, -0.2460,\n",
            "        -0.6910,  0.2241,  0.1366, -0.6425, -0.9364,  0.8674,  0.5115,  0.4715,\n",
            "        -0.0195, -1.1206, -0.7382, -0.2078, -0.3084,  0.3226,  0.0551, -0.4556,\n",
            "        -0.9575,  0.4610, -0.4741, -0.6724,  0.5813, -1.2560, -0.2587,  0.5787,\n",
            "         0.4229, -0.7130,  0.1701,  1.0722,  0.3288, -0.4900,  1.6441,  0.1121,\n",
            "         0.7037,  0.6791,  1.3215,  0.5854, -0.8697, -0.3246, -0.5486,  0.6922,\n",
            "         0.0255,  1.3960, -0.0765, -0.2021,  0.3018,  0.2524, -1.0117,  0.2877,\n",
            "         0.5397,  0.3793, -0.0650,  1.1037, -0.2339,  0.0717,  0.4808,  0.3143,\n",
            "         0.0666,  0.1947,  1.3404,  0.6896,  0.9212,  0.0698, -0.6229,  0.5238,\n",
            "        -0.6804, -0.0067,  0.5300,  0.0819,  1.3090],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "LM_head=nn.Linear(n_embd, vocab_size)\n",
        "x_lm_head=LM_head(x)\n",
        "print(f\"Shape of x after transformation {x_lm_head.shape}\")\n",
        "print(f\"Shape of the first sequence of the batch {x_lm_head[0].shape}\")\n",
        "print(f\"Shape of the first token in the first sequence of the batch {x_lm_head[0][0].shape}\")\n",
        "print(f\"The first token in the first sequence of the batch:\\n{x_lm_head[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsjDOepU9_cf"
      },
      "source": [
        "The transformed values are not probabilities, so we apply softmax to convert them into a probability distribution across 85 characters, all summing to 1. As seen below by the uniformity of these probabilities, it's clear this is the initial forward pass as the model has yet to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrxwlggm9okZ",
        "outputId": "a3cef855-3f86-4439-8c2c-d60b46c19061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0120, 0.0139, 0.0194, 0.0024, 0.0069, 0.0123, 0.0089, 0.0058, 0.0192,\n",
              "        0.0200, 0.0054, 0.0104, 0.0188, 0.0047, 0.0103, 0.0067, 0.0043, 0.0107,\n",
              "        0.0098, 0.0045, 0.0033, 0.0203, 0.0143, 0.0137, 0.0084, 0.0028, 0.0041,\n",
              "        0.0069, 0.0063, 0.0118, 0.0090, 0.0054, 0.0033, 0.0135, 0.0053, 0.0044,\n",
              "        0.0153, 0.0024, 0.0066, 0.0152, 0.0130, 0.0042, 0.0101, 0.0250, 0.0119,\n",
              "        0.0052, 0.0442, 0.0096, 0.0173, 0.0169, 0.0320, 0.0153, 0.0036, 0.0062,\n",
              "        0.0049, 0.0171, 0.0088, 0.0345, 0.0079, 0.0070, 0.0116, 0.0110, 0.0031,\n",
              "        0.0114, 0.0147, 0.0125, 0.0080, 0.0258, 0.0068, 0.0092, 0.0138, 0.0117,\n",
              "        0.0091, 0.0104, 0.0326, 0.0170, 0.0215, 0.0092, 0.0046, 0.0144, 0.0043,\n",
              "        0.0085, 0.0145, 0.0093, 0.0316], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_1_probabilities=torch.nn.functional.softmax(x_lm_head[0][0],0)\n",
        "token_1_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhnXS68t-6fD",
        "outputId": "5affb7c6-c202-44fc-9906-961b29bbe211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is the target tensor for the x tensor we are working with:\n",
            "tensor([[ 1, 72, 67,  1, 63, 66, 67, 75],\n",
            "        [53, 72,  1, 75, 57,  1, 55, 53],\n",
            "        [70, 67, 75, 66,  7,  1, 77, 57],\n",
            "        [64, 61, 72, 72, 64, 57,  1, 59]])\n",
            "The target classification of the first token of the first sequence in the batch is:\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(f\"The following is the target tensor for the x tensor we are working with:\\n{y_example}\")\n",
        "print(f\"The target classification of the first token of the first sequence in the batch is:\\n{y_example[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svk-hpMXB6SD"
      },
      "source": [
        "Now, we will calculate the cross-entropy loss between our model's predictions (`token_1_probabilities`) and the target classification of the first token, which is `1`. Cross-entropy loss is a metric in classification tasks, as it quantifies the difference between two probability distributions - the predicted probabilities and the actual distribution of the target labels.\n",
        "\n",
        "**Cross-Entropy Loss Equation:**\n",
        "\n",
        "The general formula for cross-entropy loss is:\n",
        "\n",
        "${Loss} = -\\sum_{c=1}^{M} y_{o,c} \\log(p_{o,c}) $\n",
        "\n",
        "where \\( M \\) is the number of classes, $ y_{o,c} $ is a binary indicator (0 or 1) showing whether class $ c $ is the correct classification for observation $o $, and $p_{o,c} $ is the predicted probability of observation $ o $ belonging to class $ c $.\n",
        "\n",
        "For instances where the target class is `1`, and assuming a one-hot encoded target, the equation simplifies to:\n",
        "\n",
        "${Loss} = -\\log(p_{o,1}) $\n",
        "\n",
        "Here, $ p_{o,1} $ represents the model's predicted probability that observation $o $ belongs to class `1`. This simplification occurs because the indicator function $ y_{o,c} $ is 1 for the correct class and 0 for all others, thus negating their contributions to the loss.\n",
        "\n",
        "When implementing the `torch.nn.functional.cross_entropy` we use use the logits directly (outputs from the model before applying the softmax) because the function computes the softmax internally.\n",
        "\n",
        "This code below calculates the loss for the first token of the first sequence by comparing the logits (`x_lm_head[0][0]`) directly against the actual target (`y_example[0][0]`), with both tensors unsqueezed to add a batch dimension as required by the loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embG8tXaU93L",
        "outputId": "50fb5676-218d-4683-f44b-353c49dcabdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for token 1: 4.2735772132873535\n"
          ]
        }
      ],
      "source": [
        "loss_token_1 = F.cross_entropy(x_lm_head[0][0].unsqueeze(0), y_example[0][0].unsqueeze(0))\n",
        "print(f\"Loss for token 1: {loss_token_1.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnTVF-ILGpwJ"
      },
      "source": [
        "For further intuition, see how the loss is the same when calculating it using the math module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8VGnGuYGUj0",
        "outputId": "d349ab18-667d-4770-876b-d23ece982329"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.273577026684474"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_token_1_with_equation=-math.log(token_1_probabilities[1]) #Index 1 because the target was 1\n",
        "loss_token_1_with_equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDpcMCqH_NG"
      },
      "source": [
        "To compute the loss for the entire batch, we compare each input token to its target by reshaping the tensors for compatibility with the cross-entropy loss function:\n",
        "\n",
        "- **Reshaping Logits and Targets**: The logits tensor `x_lm_head` is reshaped from `(B, T, C)` to `(B*T, C)`, and the target tensor `y_example` from `(B, T)` to `(B*T)`. This transformation treats each token as an independent instance, aligning logits with their corresponding targets. The rows of `y_example` are stacked vertically during this process. This restructuring doesn't change the data but ensures each prediction is accurately paired with its label, facilitating direct comparison and loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTQQiSUCVdo2",
        "outputId": "492a2238-d125-4e6c-8937-940ac2e0333d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Logits shape: torch.Size([32, 85])\n",
            "y_example tensor before reogranization:\n",
            "tensor([[ 1, 72, 67,  1, 63, 66, 67, 75],\n",
            "        [53, 72,  1, 75, 57,  1, 55, 53],\n",
            "        [70, 67, 75, 66,  7,  1, 77, 57],\n",
            "        [64, 61, 72, 72, 64, 57,  1, 59]])\n",
            "y_example tensor (targets) after reogranization:\n",
            "tensor([ 1, 72, 67,  1, 63, 66, 67, 75, 53, 72,  1, 75, 57,  1, 55, 53, 70, 67,\n",
            "        75, 66,  7,  1, 77, 57, 64, 61, 72, 72, 64, 57,  1, 59])\n",
            "The Targets shape: torch.Size([32])\n",
            "Loss for the first batch after the first forward pass: 4.8207879066467285\n"
          ]
        }
      ],
      "source": [
        "B,T,C= x_lm_head.shape\n",
        "logits=x_lm_head.view(B*T,C)\n",
        "print(f\"The Logits shape: {logits.shape}\")\n",
        "targets= y_example.view(B*T)\n",
        "print(f\"y_example tensor before reogranization:\\n{y_example}\")\n",
        "print(f\"y_example tensor (targets) after reogranization:\\n{targets}\")\n",
        "print(f\"The Targets shape: {targets.shape}\")\n",
        "loss=F.cross_entropy(logits,targets)\n",
        "print(f\"Loss for the first batch after the first forward pass: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1bcpfknSQ7M"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "This concludes a complete forward pass through a standard transformer decoder network. The next step is backpropagation, where gradients are computed to update the weights, followed by subsequent iterations of training to refine the model parameters based on the loss function.\n",
        "\n",
        "This tutorial aimed to provide an understanding of how a transformer is constructed by demonstrating the setup of the training dataset and detailing a forward pass through the model with a single input batch. Observing the changes in tensor dimensions and the computation of attention, offers valuable insights into how the model operates. While we cannot train a full Beatles GPT model here due to space constraints, this is the foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDlBCqhx3bYu"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyabBYMV2mz_"
      },
      "source": [
        "Karapathy, A. (2023). Building a GPT. [online] Available at: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing.Pytorch (n.d.).\n",
        "\n",
        "Cross Entropy Loss — documentation. [online] pytorch. Available at: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.Pytorch (n.d.).\n",
        "\n",
        "Softmax — documentation. [online] pytorch. Available at: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax.\n",
        "\n",
        "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I. (2017). Attention Is All You Need. [online] arXiv.org. Available at: https://arxiv.org/abs/1706.03762.\n",
        "\n",
        "Hugging Face (n.d.). Decoder models - Hugging Face NLP Course. [online] huggingface.co. Available at: https://huggingface.co/learn/nlp-course/chapter1/6."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
